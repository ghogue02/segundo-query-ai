# Implementation Specification - Defined Metrics Dashboard
**Based on:** Greg's Answers (Oct 2, 2025)
**Status:** APPROVED - Ready to Build

---

## ğŸ“‹ Approved Specifications

### 1. Default View & Filters
âœ… **Default Time Range:** All time (entire cohort)
âœ… **Real-time Updates:** Dashboard updates as filters change
âœ… **Filter Categories:**
- Time: All time, Last 7/14/30 days, Custom range, Week selection
- Builders: All (76), Top Performers, Struggling (show BOTH threshold & composite score)
- Activities: 5 meta-categories (Core Learning & Applied Work = priority)
- Tasks: Individual/Group, Conversation/Basic

### 2. Builder Segmentation (Dual Display)
**Show BOTH approaches side-by-side for comparison:**

#### Approach A: Threshold-Based
```
Struggling Builder:
- Task completion < 50% OR Attendance < 70%

Top Performer:
- Task completion > 90% AND Attendance > 90%
```

#### Approach B: Composite Score
```
Engagement Score = (Attendance Ã— 0.3) + (Task Completion Ã— 0.5) + (Quality Ã— 0.2)

Struggling: Score < 40
Top Performer: Score > 80
```

**UI:** Toggle between views or show both in separate cards

### 3. Activity Categories (5 Meta-Groups)
**Priority Order:**
1. **Core Learning** â­ (Learning, Practice, Research, Self learning)
2. **Applied Work** â­ (Building/Build, Testing, Assessment)
3. **Collaboration** (Group Activity, Team Building, Presentation)
4. **Reflection** (Reflection/Reflecton, Individual)
5. **Other** (Break, Celebration, Networking, Planning, Preparation, Professional Development)

### 4. Hypothesis Charts (All 7)
Build all 7, can delete/modify later:

**H1: Attendance Drives Task Completion**
- Chart: Scatter plot (Attendance % vs Completion %)
- Show: Correlation coefficient

**H2: Early Engagement Predicts Success**
- Chart: Scatter plot (Week 1 submissions vs Total submissions)
- Show: Trend line

**H3: Activity Type Preference**
- Chart: Radar chart per builder segment
- Show: Completion % by category

**H4: Improvement Trajectory**
- Chart: Line chart (Week-over-week completion %)
- Show: Trend direction (improving/declining)

**H5: Weekend Work Patterns**
- Chart: Bar chart (Sat/Sun submissions vs Mon-Wed)
- Show: Completion rate difference

**H6: Peer Influence**
- Chart: TBD (need table group data - explore if available)
- Show: Group performance correlation

**H7: Task Difficulty Analysis**
- Chart: Histogram (Task completion rate distribution)
- Show: Tasks with <70% completion (redesign candidates)

### 5. KPI Cards (5 Cards)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Active       â”‚ Active       â”‚ Task         â”‚ Attendance   â”‚ Need         â”‚
â”‚ Builders     â”‚ Builders     â”‚ Completion   â”‚ Rate         â”‚ Intervention â”‚
â”‚ TODAY        â”‚ PRIOR DAY    â”‚ This Week    â”‚ (7 class avg)â”‚              â”‚
â”‚              â”‚              â”‚              â”‚              â”‚              â”‚
â”‚ 68/76        â”‚ 71/76        â”‚ 92%          â”‚ 87%          â”‚ 5 builders   â”‚
â”‚ (89%)        â”‚ (93%)        â”‚ â†‘ 4% vs last â”‚ â†“ 4% vs last â”‚ (see list)   â”‚
â”‚              â”‚              â”‚              â”‚              â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical:** 7-day average = 7 CLASS days only (Sat-Wed, excluding Thu/Fri)

### 6. Quality Metrics (Both A & B)
**Show BOTH:**

**A) Average Quality Score (Top KPI or Card)**
```
Overall Quality: 78/100
(Based on rubric scoring from BigQuery)
```

**B) Quality Breakdown (Radar Chart)**
```
Technical Skills:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 82%
Business Value:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   65%
Project Management:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  75%
Research Skills:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 80%
Critical Thinking:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  72%
Risk Assessment:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   68%
```

### 7. Task Difficulty Scoring
âŒ **Not implemented** - Do not label task difficulty

### 8. Chart Refresh
âœ… **Every 5 minutes**
âœ… **Display:** "Last refreshed: 2 minutes ago" (timestamp in top-right corner)
âœ… **Manual refresh button:** "â†» Refresh Now"

### 9. Export Features
âŒ **Skip for MVP** - Will add in Phase 2

---

## ğŸ” CRITICAL: Task Completion vs Submissions

**Two Different Metrics:**

### 1. Submission Count
```sql
-- Simple: Did they submit anything?
SELECT COUNT(*) FROM task_submissions
WHERE task_id = X AND user_id = Y
```

### 2. Task Completion (Interaction + Quality)
**Dual criteria:**
- **a) Did they interact?** (EXISTS check)
- **b) Quality of text interactions** (analyze submission content)

---

## ğŸ§  Quality Analysis at Scale - Proposed Solution

### The Challenge
Need to measure quality of text submissions WITHOUT manual review at scale.

### Proposed Approach: AI-Powered Quality Scoring

#### Option 1: Real-time Analysis (As Submitted)
```javascript
// When builder submits task
async function analyzeSubmission(submission) {
  const prompt = `
    Analyze this task submission quality on a scale of 0-100:

    Task: ${task.description}
    Submission: ${submission.text}

    Evaluate:
    1. Completeness: Did they address the task requirements?
    2. Depth: Is there evidence of critical thinking?
    3. Effort: Does it show genuine engagement vs minimal effort?

    Return JSON: { score: 0-100, reasoning: "brief explanation" }
  `;

  const analysis = await claude.analyze(prompt);

  return {
    interaction: true, // They submitted
    quality_score: analysis.score,
    completion_status: analysis.score >= 60 ? 'completed' : 'incomplete'
  };
}
```

**Pros:**
- âœ… Real-time feedback
- âœ… Scales automatically
- âœ… Nuanced quality assessment

**Cons:**
- âš ï¸ API costs (1 call per submission)
- âš ï¸ Need to validate AI scoring accuracy

---

#### Option 2: Batch Analysis (Nightly)
```javascript
// Run nightly on all submissions from past 24 hours
async function batchAnalyzeSubmissions() {
  const submissions = await getRecentSubmissions(24); // hours

  const results = await Promise.all(
    submissions.map(async (sub) => {
      // Use same prompt as Option 1
      const analysis = await claude.analyze(buildPrompt(sub));

      // Update database
      await updateSubmissionQuality(sub.id, {
        quality_score: analysis.score,
        analyzed_at: new Date()
      });
    })
  );
}
```

**Pros:**
- âœ… Lower cost (batch processing)
- âœ… Can review/validate before showing to users

**Cons:**
- âš ï¸ Not real-time (up to 24h delay)

---

#### Option 3: Hybrid (Smart Triggers)
```javascript
// Analyze immediately if:
// 1. Task is marked "should_analyze: true"
// 2. Submission length > 100 chars
// Otherwise, batch process

async function smartAnalysis(submission, task) {
  if (task.should_analyze || submission.text.length > 100) {
    // Real-time analysis
    return await analyzeSubmission(submission);
  } else {
    // Queue for batch processing
    await queueForBatchAnalysis(submission.id);
    return { interaction: true, quality_score: null }; // Pending
  }
}
```

**Pros:**
- âœ… Best of both worlds
- âœ… Cost-effective
- âœ… Real-time for important tasks

**Cons:**
- âš ï¸ More complex logic

---

### Quality Scoring Rubric (For AI Analysis)

```javascript
const qualityCriteria = {
  completeness: {
    weight: 0.4,
    scale: [
      { score: 0-20, desc: "Did not address task requirements" },
      { score: 21-40, desc: "Partially addressed requirements" },
      { score: 41-60, desc: "Addressed most requirements" },
      { score: 61-80, desc: "Fully addressed requirements" },
      { score: 81-100, desc: "Exceeded requirements with extras" }
    ]
  },
  depth: {
    weight: 0.3,
    scale: [
      { score: 0-20, desc: "Surface-level response, no critical thinking" },
      { score: 21-40, desc: "Basic understanding shown" },
      { score: 41-60, desc: "Demonstrates understanding and some analysis" },
      { score: 61-80, desc: "Deep analysis with examples" },
      { score: 81-100, desc: "Exceptional insight and original thinking" }
    ]
  },
  effort: {
    weight: 0.3,
    scale: [
      { score: 0-20, desc: "Minimal effort (e.g., 'I don't know')" },
      { score: 21-40, desc: "Some effort but incomplete" },
      { score: 41-60, desc: "Genuine engagement evident" },
      { score: 61-80, desc: "Significant effort and care" },
      { score: 81-100, desc: "Exceptional effort and polish" }
    ]
  }
};

// Final Score = (Completeness Ã— 0.4) + (Depth Ã— 0.3) + (Effort Ã— 0.3)
```

---

### UI Display: Task Completion with Quality

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task Completion Breakdown                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚ âœ… Completed (High Quality)      62 tasks   â”‚
â”‚    â””â”€ Quality Score â‰¥ 70         (82%)      â”‚
â”‚                                              â”‚
â”‚ âš ï¸  Completed (Low Quality)       10 tasks   â”‚
â”‚    â””â”€ Quality Score < 70         (13%)      â”‚
â”‚                                              â”‚
â”‚ ğŸ“ Interacted (Pending Analysis)  3 tasks    â”‚
â”‚    â””â”€ Awaiting quality review    (4%)       â”‚
â”‚                                              â”‚
â”‚ âŒ Not Started                    1 task     â”‚
â”‚    â””â”€ No submissions              (1%)      â”‚
â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Overall Completion Rate: 99% (75/76 tasks)  â”‚
â”‚ High-Quality Completion: 82% (62/76 tasks)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â“ Decision Needed: Which Quality Analysis Option?

**Greg, which approach do you prefer?**

**A) Option 1:** Real-time analysis (every submission)
- Immediate feedback, higher cost

**B) Option 2:** Batch nightly analysis
- Delayed feedback, lower cost

**C) Option 3:** Hybrid (smart triggers)
- Best balance, more complex

**D) Something else:** (describe)

**My Recommendation:** Option C (Hybrid) - analyze important tasks real-time, batch process the rest

**Your Answer:** _____

---

## ğŸ“Š Database Schema Updates Needed

### New Table: `submission_quality_analysis`
```sql
CREATE TABLE submission_quality_analysis (
  id SERIAL PRIMARY KEY,
  submission_id INTEGER REFERENCES task_submissions(id),
  quality_score INTEGER, -- 0-100
  completeness_score INTEGER, -- 0-100
  depth_score INTEGER, -- 0-100
  effort_score INTEGER, -- 0-100
  analysis_reasoning TEXT,
  analyzed_at TIMESTAMP,
  analysis_version VARCHAR(50), -- Track which AI model/prompt version
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_submission_quality ON submission_quality_analysis(submission_id);
CREATE INDEX idx_quality_score ON submission_quality_analysis(quality_score);
```

### Update `task_submissions` Table
```sql
-- Add quality flag
ALTER TABLE task_submissions
ADD COLUMN has_quality_analysis BOOLEAN DEFAULT FALSE,
ADD COLUMN quality_score INTEGER; -- Cache for performance
```

---

## ğŸ—ï¸ Component Architecture

### New Components to Build

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ metrics-dashboard/
â”‚   â”‚   â”œâ”€â”€ MetricsTab.tsx (main container)
â”‚   â”‚   â”œâ”€â”€ FilterSidebar.tsx
â”‚   â”‚   â”œâ”€â”€ KPICards.tsx
â”‚   â”‚   â”œâ”€â”€ HypothesisCharts/
â”‚   â”‚   â”‚   â”œâ”€â”€ H1_AttendanceVsCompletion.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ H2_EarlyEngagement.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ H3_ActivityPreference.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ H4_ImprovementTrajectory.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ H5_WeekendPatterns.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ H6_PeerInfluence.tsx
â”‚   â”‚   â”‚   â””â”€â”€ H7_TaskDifficulty.tsx
â”‚   â”‚   â”œâ”€â”€ QualityMetrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ QualityOverview.tsx (avg score)
â”‚   â”‚   â”‚   â””â”€â”€ QualityRadarChart.tsx (by rubric)
â”‚   â”‚   â””â”€â”€ RefreshIndicator.tsx (last updated timestamp)
â”‚   â”‚
â”‚   â””â”€â”€ terminology-legend/
â”‚       â”œâ”€â”€ LegendTab.tsx
â”‚       â””â”€â”€ MetricDefinition.tsx
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ quality-analysis.ts (AI scoring logic)
â”‚   â””â”€â”€ metrics-calculations.ts (composite scores, etc.)
â”‚
â””â”€â”€ api/
    â”œâ”€â”€ metrics/
    â”‚   â”œâ”€â”€ kpis.ts
    â”‚   â”œâ”€â”€ hypotheses.ts
    â”‚   â””â”€â”€ quality-scores.ts
    â””â”€â”€ quality/
        â”œâ”€â”€ analyze.ts (real-time analysis endpoint)
        â””â”€â”€ batch.ts (batch processing)
```

---

## â±ï¸ Implementation Timeline

### Week 1: Foundation
- [ ] Database schema updates (submission_quality_analysis table)
- [ ] Quality analysis system (pick Option A/B/C)
- [ ] Filter sidebar component
- [ ] KPI cards with 7-day class average fix

### Week 2: Hypothesis Charts
- [ ] H1: Attendance vs Completion (scatter)
- [ ] H2: Early engagement (scatter)
- [ ] H4: Improvement trajectory (line)
- [ ] H7: Task difficulty (histogram)

### Week 3: Quality & Advanced
- [ ] Quality overview card
- [ ] Quality radar chart (rubric breakdown)
- [ ] H3, H5, H6 charts
- [ ] Dual segmentation (threshold vs composite)

### Week 4: Polish & Test
- [ ] Refresh indicator (5 min auto + manual)
- [ ] Real-time filter updates
- [ ] Terminology legend tab
- [ ] Testing with facilitators

---

## ğŸš¨ Blockers to Resolve

1. **Quality Analysis Approach** â†’ Need your decision (A/B/C/D)
2. **Composite Score Weights** â†’ Confirm: Attendance (0.3) + Completion (0.5) + Quality (0.2)
3. **Table Group Data** â†’ Do we have this for H6? (Check database)
4. **BigQuery Integration** â†’ Get credentials for quality rubric data

---

## âœ… Ready to Start?

**Answer the quality analysis question above and I'll begin building immediately.**

Which option: **A / B / C / D**

Also confirm:
- [ ] Composite score weights look good (0.3 / 0.5 / 0.2)?
- [ ] Okay to analyze submission text content with AI?
- [ ] Any privacy concerns with quality scoring?
