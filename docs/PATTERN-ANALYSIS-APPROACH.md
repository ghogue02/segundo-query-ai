# Pattern Analysis Approach - Task Completion Quality
**Based on:** Greg's Clarification (Oct 2, 2025)
**Key Insight:** Use existing BigQuery scores + analyze cohort-level text patterns

---

## ğŸ¯ Corrected Understanding

### âŒ WRONG Approach (What I Was Proposing)
- Re-score individual submissions with AI
- Create new quality metrics
- Duplicate existing BigQuery data

### âœ… RIGHT Approach (What You Want)
- **Use existing quality scores from BigQuery** (don't re-score)
- **Analyze text patterns across entire cohort** (aggregate analysis)
- **Find insights from large sample size** (76 builders Ã— 107 tasks)

---

## ğŸ“Š Two-Part Quality System

### Part 1: Individual Quality Scores (BigQuery)
**Source:** Already exists in `assessment_feedback` table
```sql
-- Pull existing scores
SELECT
  user_id,
  task_id,
  rubric_scores, -- Technical Skills, Business Value, etc.
  overall_score,
  feedback_text
FROM assessment_feedback
WHERE cohort = 'September 2025'
```

**Use for:**
- Individual builder quality metrics
- Quality radar chart (by rubric category)
- Top performer identification (composite score with quality component)

---

### Part 2: Cohort Pattern Analysis (NEW - What You Want)
**Analyze text submissions at aggregate level to answer:**

1. **How did the cohort interact with this task?**
   - What approaches did most builders take?
   - What common patterns emerge?
   - Where did most builders get stuck?

2. **What does the text data reveal at scale?**
   - Depth of responses (superficial vs thorough)
   - Common misconceptions
   - Evidence of understanding vs confusion
   - Copy-paste patterns (similar wording = potential collaboration or AI over-reliance)

3. **Which tasks need redesign?**
   - Tasks with low-quality patterns across cohort
   - Tasks where text shows confusion/frustration
   - Tasks where submissions are too short/generic

---

## ğŸ” Pattern Analysis Implementation

### Approach: Aggregate Task Analysis

**Run once per task (not per submission):**

```javascript
async function analyzeTaskPatterns(taskId) {
  // Get ALL submissions for this task across cohort
  const submissions = await db.query(`
    SELECT
      ts.user_id,
      ts.thread_content, -- All text interactions
      ts.created_at,
      u.first_name,
      u.last_name
    FROM task_submissions ts
    JOIN users u ON ts.user_id = u.user_id
    WHERE ts.task_id = $1
      AND u.cohort = 'September 2025'
      AND u.active = true
      AND u.user_id NOT IN (129,5,240,324,325,326,9,327,329,331,330,328,332)
  `, [taskId]);

  // Aggregate analysis prompt
  const prompt = `
    Analyze these ${submissions.length} submissions for Task "${task.title}":

    SUBMISSIONS:
    ${submissions.map((s, i) => `
      Builder ${i + 1}: ${s.thread_content}
    `).join('\n---\n')}

    Provide cohort-level pattern analysis:

    1. INTERACTION PATTERNS:
       - What approaches did most builders take?
       - Common strategies or methods used
       - Range of response lengths (min/max/avg)

    2. UNDERSTANDING INDICATORS:
       - % showing deep understanding
       - % showing partial understanding
       - % showing confusion/struggling
       - Common misconceptions

    3. QUALITY DISTRIBUTION:
       - High quality responses: X%
       - Medium quality: X%
       - Low quality: X%
       - Explanation of what differentiates them

    4. RED FLAGS:
       - Tasks where cohort struggled
       - Patterns indicating task redesign needed
       - Evidence of copy-paste or over-reliance on AI
       - Unusually short or generic responses

    5. ACTIONABLE INSIGHTS:
       - What this tells us about the task design
       - Recommendations for facilitators
       - Curriculum improvement suggestions

    Return JSON format.
  `;

  const analysis = await claude.analyze(prompt);

  // Store aggregate analysis (not per-builder)
  await db.query(`
    INSERT INTO task_pattern_analysis
    (task_id, total_submissions, analysis_results, analyzed_at)
    VALUES ($1, $2, $3, NOW())
  `, [taskId, submissions.length, analysis]);

  return analysis;
}
```

---

## ğŸ“Š Dashboard Display: Pattern Insights

### Task-Level View
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task #1: "Build Your First MVP"                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚ ğŸ“Š Completion Stats                                         â”‚
â”‚   âœ… Submitted: 72/76 (95%)                                 â”‚
â”‚   â­ Quality (BigQuery): Avg 78/100                         â”‚
â”‚                                                              â”‚
â”‚ ğŸ” Cohort Patterns (AI Analysis)                            â”‚
â”‚                                                              â”‚
â”‚   Understanding Distribution:                                â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Deep Understanding     42% (32 builders)     â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    Partial Understanding   35% (27 builders)     â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆ        Struggling              18% (13 builders)     â”‚
â”‚                                                              â”‚
â”‚   Common Approaches:                                         â”‚
â”‚   â€¢ 68% focused on web-based MVPs                           â”‚
â”‚   â€¢ 45% mentioned user validation                           â”‚
â”‚   â€¢ 32% discussed technical feasibility                     â”‚
â”‚                                                              â”‚
â”‚   âš ï¸  Red Flags:                                            â”‚
â”‚   â€¢ 12 builders had unusually short responses (<50 words)   â”‚
â”‚   â€¢ Common confusion about "minimum" in MVP                 â”‚
â”‚   â€¢ 8 responses show similar wording (possible AI overuse)  â”‚
â”‚                                                              â”‚
â”‚   ğŸ’¡ Recommendations:                                        â”‚
â”‚   â€¢ Add examples of good vs bad MVPs in task intro          â”‚
â”‚   â€¢ Clarify "minimum" vs "incomplete"                       â”‚
â”‚   â€¢ Check in with 13 struggling builders                    â”‚
â”‚                                                              â”‚
â”‚ [View All 72 Submissions] [See Detailed Analysis]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—„ï¸ Database Schema

### New Table: `task_pattern_analysis`
```sql
CREATE TABLE task_pattern_analysis (
  id SERIAL PRIMARY KEY,
  task_id INTEGER REFERENCES tasks(id),
  cohort VARCHAR(50),
  total_submissions INTEGER,

  -- Pattern insights (JSON)
  interaction_patterns JSONB,
  -- {
  --   common_approaches: ["web-based", "mobile app", ...],
  --   avg_word_count: 245,
  --   response_length_range: [12, 850]
  -- }

  understanding_distribution JSONB,
  -- {
  --   deep: { count: 32, percentage: 42 },
  --   partial: { count: 27, percentage: 35 },
  --   struggling: { count: 13, percentage: 18 }
  -- }

  quality_patterns JSONB,
  -- {
  --   high: 42,
  --   medium: 35,
  --   low: 18,
  --   differentiators: ["depth of examples", "critical analysis", ...]
  -- }

  red_flags JSONB,
  -- {
  --   short_responses: 12,
  --   similar_wording_groups: 3,
  --   common_misconceptions: ["confusion about MVP definition", ...]
  -- }

  recommendations TEXT[],
  -- ["Add MVP examples", "Clarify minimum definition", ...]

  analyzed_at TIMESTAMP,
  analysis_version VARCHAR(50),

  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_task_pattern_cohort ON task_pattern_analysis(task_id, cohort);
```

---

## ğŸ¯ Metrics Dashboard Integration

### New Charts Using Pattern Data

#### 1. Task Quality Heatmap
```
Shows which tasks have strong vs weak cohort patterns

         Week 1    Week 2    Week 3    Week 4
Task 1   ğŸŸ¢ 89%
Task 2   ğŸŸ¡ 72%
Task 3   ğŸŸ¢ 91%
Task 4   ğŸ”´ 45%   â† Red flag: Cohort struggled
Task 5            ğŸŸ¢ 88%

Green: >80% deep understanding
Yellow: 60-80% mixed understanding
Red: <60% struggling
```

#### 2. Common Misconceptions Tracker
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Top 5 Misconceptions Across Cohort      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. "MVP = incomplete product"           â”‚
â”‚    ğŸ“Š Appeared in 23 submissions        â”‚
â”‚    ğŸ¯ Task: Build Your First MVP        â”‚
â”‚                                          â”‚
â”‚ 2. "User validation = surveys only"     â”‚
â”‚    ğŸ“Š Appeared in 18 submissions        â”‚
â”‚    ğŸ¯ Task: Customer Discovery          â”‚
â”‚                                          â”‚
â”‚ 3. "AI always gives correct answers"    â”‚
â”‚    ğŸ“Š Appeared in 15 submissions        â”‚
â”‚    ğŸ¯ Task: Working with AI             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Response Quality Distribution
```
Shows how cohort quality varies by task category

Core Learning Tasks:
Deep â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 65%
Partial â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   35%

Applied Work Tasks:
Deep â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 48%
Partial â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 52%

Reflection Tasks:
Deep â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 58%
Partial â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 42%
```

#### 4. Text Pattern Anomalies
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flagged for Review                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”´ Task #12: 8 similar responses        â”‚
â”‚    Possible collaboration/AI overuse     â”‚
â”‚    [View Submissions]                    â”‚
â”‚                                          â”‚
â”‚ âš ï¸  Task #7: Avg 32 words (too short)   â”‚
â”‚    Builders may not understand prompt    â”‚
â”‚    [Review Task Design]                  â”‚
â”‚                                          â”‚
â”‚ ğŸŸ¡ Task #19: 45% show confusion         â”‚
â”‚    Common misconception detected         â”‚
â”‚    [See Pattern Analysis]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Analysis Frequency

### Real-time: Individual Metrics
- Submission count: Instant
- Existing BigQuery quality scores: Pull on demand

### Batch Analysis: Pattern Insights
**Run nightly or weekly (configurable):**
- Analyze tasks with >10 new submissions since last run
- Update pattern analysis for modified tasks
- Generate cohort-level insights

**Reasoning:**
- Patterns need critical mass (10+ submissions)
- Daily updates sufficient for curriculum insights
- Lower cost than real-time analysis

---

## ğŸ¯ Key Hypotheses Using Pattern Data

### H7 (Enhanced): Task Quality Patterns
**Question:** Which tasks consistently produce low-quality cohort responses?

**Chart:** Scatter plot
- X-axis: Task completion rate
- Y-axis: % of cohort with "deep understanding"
- Color: Red (redesign needed), Yellow (review), Green (working well)

**Insight:** Tasks in bottom-left quadrant (low completion + low understanding) need redesign

### H8 (NEW): Understanding Progression
**Question:** Does cohort understanding depth improve over time?

**Chart:** Line chart
- X-axis: Week number
- Y-axis: % of cohort with deep understanding
- Shows learning trajectory

### H9 (NEW): Task Type Effectiveness
**Question:** Which task categories produce best understanding?

**Chart:** Bar chart comparing:
- Learning tasks
- Building tasks
- Reflection tasks
- Group activities

By "deep understanding %" from pattern analysis

---

## âœ… Confirmed Approach

### What We're Building:

1. **Pull existing BigQuery scores** (don't re-score)
2. **Aggregate pattern analysis per task** (cohort-level insights)
3. **Dashboard shows:**
   - Individual quality scores (from BigQuery)
   - Cohort patterns (from text analysis)
   - Task effectiveness heatmap
   - Common misconceptions
   - Red flags for curriculum redesign

### What We're NOT Building:
- âŒ Individual submission re-scoring
- âŒ Duplicate quality metrics
- âŒ Real-time per-submission analysis

---

## ğŸš€ Implementation Steps

### Phase 1: Data Integration (Week 1)
- [ ] Connect to BigQuery for existing quality scores
- [ ] Create `task_pattern_analysis` table
- [ ] Build pattern analysis script

### Phase 2: Pattern Analysis (Week 2)
- [ ] Implement aggregate text analysis
- [ ] Run on all September tasks
- [ ] Validate insights with facilitators

### Phase 3: Dashboard Integration (Week 3)
- [ ] Task quality heatmap
- [ ] Common misconceptions tracker
- [ ] Response quality distribution
- [ ] Anomaly detection alerts

### Phase 4: Automation (Week 4)
- [ ] Nightly batch analysis
- [ ] Alert system for red flags
- [ ] Pattern trend tracking

---

## â“ Final Confirmation

**This approach correct?**
- âœ… Use existing BigQuery quality scores
- âœ… Analyze text patterns at cohort level (aggregate)
- âœ… Find insights from large sample size (76 builders)
- âœ… Identify which tasks need redesign
- âœ… Track common misconceptions
- âœ… Flag anomalies (similar responses, too short, etc.)

**Confirm:** YES / Need adjustments: _____

**Also need:**
- BigQuery credentials for quality scores table
- Confirm table name: `assessment_feedback` or different?
- Should pattern analysis run nightly or weekly?

---

**Ready to proceed with this approach?** ğŸš€
